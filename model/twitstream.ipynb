{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model for Analysing Twitter Sentiments\n",
    "\n",
    "In this notebook we build a classification model to perform sentiment analysis on tweets. Sentiment Analysis means we know the *Polarity* of a tweet whether it is *Positive* or *Negative*.Once the model is built it will be able to do the analysis in real-time through use of a Twitter API.\n",
    "\n",
    "###### Steps to be executed: \n",
    "\n",
    "1. Inorder to process tweets, install the NLTK package (Natural Language Toolkit) and all other neccessary libraries.\n",
    "2. Load all datasets of tweets that express positive and negative sentiments\n",
    "3. From each tweet, we have to perfom Tokenization, normalization, and removal of noise and stopwords.\n",
    "4. In the particular dataset the Word Density will then be determined.\n",
    "5. Assemble the cleaned data into a dataset and split it into a training and testing sets.\n",
    "6. The training of the Naive Bayes classification model and its validation.\n",
    "7. Save the model into binary format.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step1: Install NLTK and Dependencies\n",
    "\n",
    "We will need to install Jupyter and the dependencies of this project.\n",
    "\n",
    "Following steps are not mandatory:\n",
    "\n",
    "```bash\n",
    "# In-order to avoid dependency conflicts, create a virtual environment to seperate this project from other Python projects\n",
    "virtualenv -p python3 venv\n",
    "\n",
    "# Activate your virtual env, and you will see a (venv) before your usual terminal prompt\n",
    "source venv/bin/activate\n",
    "\n",
    "# To install jupyter in the environment\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "Installation:\n",
    "\n",
    "```bash\n",
    "# Install the single main dependency\n",
    "pip install nltk==3.4.5\n",
    "```\n",
    "We still need to install libraries that will aid in the processing of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')        # It helps to tokenize sentences into single words, since it contains a pre-trained model\n",
    "nltk.download('wordnet')      # Lexical database of use during normalization\n",
    "nltk.download('averaged_perceptron_tagger')    # Tagger to find the nature of words i.e whether it is verb, noun, ...\n",
    "nltk.download('stopwords')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a local download and storage of datasets\n",
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative_tweets.json', 'positive_tweets.json', 'tweets.20150430-223406.json']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import twitter_samples\n",
    "\n",
    "# Check availabe fields in our dataset\n",
    "twitter_samples.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the labelled data as our training set\n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "# Load the unlabelled data as the test set\n",
    "text = twitter_samples.strings('tweets.20150430-223406.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Tokenization, Normalization, and Removal of Noise and Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "In simple terms it is splitting sentences into single words known as *tokens*, including emojis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of a positive tweet:\n",
      "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "\n",
      "Tokens:\n",
      "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'being', 'top', 'engaged', 'members', 'in', 'my', 'community', 'this', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# Create an instance of a tweet tokenizer that will preserve each word (or token) as it is\n",
    "tweet_tokenizer = TweetTokenizer(\n",
    "    preserve_case = True,\n",
    "    reduce_len    = False,\n",
    "    strip_handles = False)\n",
    "\n",
    "tokens_positive = [tweet_tokenizer.tokenize(p) for p in positive_tweets]\n",
    "tokens_negative = [tweet_tokenizer.tokenize(n) for n in negative_tweets]\n",
    "\n",
    "print(\"An Example of a positive tweet:\\n{}\\n\".format(positive_tweets[0]))\n",
    "print(\"Tokens:\\n{}\".format(tokens_positive[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "In this stage we bring words to their original form. We will impliment Lemmatization as a normalization process.\n",
    "\n",
    "We will need to find the nature of each word by maaking use of a tagger:\n",
    "- NNP: Noun, proper, singular\n",
    "- NN: Noun, common, singular or mass\n",
    "- IN: Preposition or conjunction, subordinating\n",
    "- VBG: Verb, gerund or present participle\n",
    "- VBN: Verb, past participle\n",
    "- JJ: adjective ‘big’\n",
    "- JJR: adjective, comparative ‘bigger’\n",
    "- JJS: adjective, superlative ‘biggest’\n",
    "- ...\n",
    "\n",
    "After getting the types (Verb, noun, or others), we can extract the lemma of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#FollowFriday', 'JJ'),\n",
       " ('@France_Inte', 'NNP'),\n",
       " ('@PKuchly57', 'NNP'),\n",
       " ('@Milipol_Paris', 'NNP'),\n",
       " ('for', 'IN'),\n",
       " ('being', 'VBG'),\n",
       " ('top', 'JJ'),\n",
       " ('engaged', 'VBN'),\n",
       " ('members', 'NNS'),\n",
       " ('in', 'IN'),\n",
       " ('my', 'PRP$'),\n",
       " ('community', 'NN'),\n",
       " ('this', 'DT'),\n",
       " ('week', 'NN'),\n",
       " (':)', 'NN')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import pos_tag    #This is part-of-speech tagger\n",
    "\n",
    "tags_positive = [pos_tag(p) for p in tokens_positive]\n",
    "tags_negative = [pos_tag(n) for n in tokens_negative]\n",
    "\n",
    "# printing\n",
    "tags_positive[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of a positive tweet:\n",
      "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "\n",
      "Lemmatized:\n",
      "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'be', 'top', 'engage', 'member', 'in', 'my', 'community', 'this', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# We only need to know the type i.e whether it is Noun, Verb, or others for each word\n",
    "def _tag2type(tag):\n",
    "    '''\n",
    "    Take a tag and return a type.\n",
    "    return 'n' for noun, 'v' for verb, and 'a' for any\n",
    "    '''\n",
    "    if tag.startswith('NN'):\n",
    "        return 'n'\n",
    "    elif tag.startswith('VB'):\n",
    "        return 'v'\n",
    "    else:\n",
    "        return 'a'\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemma_positive = [[lemmatizer.lemmatize(word, _tag2type(tag)) for (word, tag) in tags] for tags in tags_positive]\n",
    "lemma_negative = [[lemmatizer.lemmatize(word, _tag2type(tag)) for (word, tag) in tags] for tags in tags_negative]\n",
    "\n",
    "\n",
    "print(\"Example of a positive tweet:\\n{}\\n\".format(positive_tweets[0]))\n",
    "print(\"Lemmatized format:\\n{}\".format(lemma_positive[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice that the verb *being* is converted to *be*, and the noun *members* to *member*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## De-noising or Noise Reduction on our data\n",
    "\n",
    "At this stage we consider the following as noise:\n",
    "1. Stopwords: Words such as \"a\", \"the\", and \"it\", are the most common words in a language and they generally don't convey a meaning, except in special cases.\n",
    "2. Hyperlinks:Inorder to shorten hyperlinks Twitter uses t.co, this renders information carried by URLs to be of no value.\n",
    "3. Mentions: Whenever usernames and pages start with a @ sympol we classify it as a mention.\n",
    "4. Punctuation: To keep everything simple we remove all punctuation as it complicates text processing.\n",
    "\n",
    "We are going to make use of the dictionary *Stopwords* from NLTK, together with regular expressions to de-noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "# print\n",
    "stopwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of a positive tweet:\n",
      "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "\n",
      "Denoised:\n",
      "['#followfriday', 'top', 'engage', 'member', 'community', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "\n",
    "def _is_noise(word):\n",
    "    pattern = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+|(@[A-Za-z0-9_]+)'\n",
    "    return word in punctuation \\\n",
    "        or word.lower() in stopwords \\\n",
    "        or re.search(pattern, word, re.IGNORECASE) != None\n",
    "\n",
    "denoised_positive = [[p.lower() for p in _list if not _is_noise(p)] for _list in lemma_positive]\n",
    "denoised_negative = [[n.lower() for n in _list if not _is_noise(n)] for _list in lemma_negative]\n",
    "\n",
    "print(\"Example of a positive tweet:\\n{}\\n\".format(positive_tweets[0]))\n",
    "print(\"Denoised:\\n{}\".format(denoised_positive[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Determination of our Dataset Word Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 most common words in a set of positive tweets:\n",
      "[(':)', 3691), (':-)', 701), (':d', 658), ('thanks', 388), ('follow', 357), ('love', 333), ('...', 290), ('good', 283), ('get', 263), ('thank', 253)]\n",
      "\n",
      "The 10 most common words in a set of negative tweets:\n",
      "[(':(', 4585), (':-(', 501), (\"i'm\", 343), ('...', 332), ('get', 325), ('miss', 291), ('go', 275), ('please', 275), ('want', 246), ('like', 218)]\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "def get_all_words(tokens_list):\n",
    "    '''\n",
    "    This generator function gets a flat mapping of all words in the dataset.\n",
    "    \n",
    "    @arg tokens_list: it is A 2-D list of (preferably cleaned) tokens\n",
    "    @return A list of all words\n",
    "    '''\n",
    "    for tokens in tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token\n",
    "\n",
    "all_pos_words = get_all_words(denoised_positive)\n",
    "all_neg_words = get_all_words(denoised_negative)\n",
    "\n",
    "freq_dist_pos = FreqDist(all_pos_words)\n",
    "freq_dist_neg = FreqDist(all_neg_words)\n",
    "\n",
    "print(\"The 10 most common words in a set of positive tweets:\\n{}\\n\".format(freq_dist_pos.most_common(10)))\n",
    "print(\"The 10 most common words in a set of negative tweets:\\n{}\".format(freq_dist_neg.most_common(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Prepare Training and Testing Datasets\n",
    "\n",
    "Our dataset is split into a training set for building the model, and a testing set for testing the performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_for_model(tokens_list):\n",
    "    '''\n",
    "    Generator function that associates a boolean 'True' to each token in a list of tokens,\n",
    "    which represents the label of each token.\n",
    "        \n",
    "    @arg tokens_list a 2-D list of (preferably cleaned) tokens\n",
    "    @return A 2-D list of tuples (original_token, True) containing the unaltered token and a boolean label\n",
    "    '''\n",
    "    for tweet_tokens in tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)\n",
    "\n",
    "positive_tokens_for_model = get_tweets_for_model(denoised_positive)\n",
    "negative_tokens_for_model = get_tweets_for_model(denoised_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "TRAIN_SIZE_RATIO = 0.7    # We use 70% as a training set\n",
    "\n",
    "positive_dataset = [(tweet_dict, \"Positive\") for tweet_dict in positive_tokens_for_model]\n",
    "negative_dataset = [(tweet_dict, \"Negative\") for tweet_dict in negative_tokens_for_model]\n",
    "\n",
    "# Merge the positive and negative sets, then shuffle to avoid any bias\n",
    "# that could come from the arrangement of tweets.\n",
    "dataset = positive_dataset + negative_dataset\n",
    "random.shuffle(dataset)\n",
    "\n",
    "train_data = dataset[: round(len(dataset) * TRAIN_SIZE_RATIO)]\n",
    "test_data = dataset[round(len(dataset) * TRAIN_SIZE_RATIO) :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Training our datasets\n",
    "\n",
    "We will use a Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy is:0.9995714285714286\n",
      "\n",
      "Testing accuracy is:0.9953333333333333\n",
      "\n",
      "Most Informative Features\n",
      "                      :) = True           Positi : Negati =    988.2 : 1.0\n",
      "                     sad = True           Negati : Positi =     31.1 : 1.0\n",
      "                follower = True           Positi : Negati =     23.6 : 1.0\n",
      "                     bam = True           Positi : Negati =     21.9 : 1.0\n",
      "                    glad = True           Positi : Negati =     19.8 : 1.0\n",
      "                     x15 = True           Negati : Positi =     16.9 : 1.0\n",
      "                 welcome = True           Positi : Negati =     15.1 : 1.0\n",
      "               community = True           Positi : Negati =     14.5 : 1.0\n",
      "                     ugh = True           Negati : Positi =     12.9 : 1.0\n",
      "                    dont = True           Negati : Positi =     12.6 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "print(\"Training accuracy is:{}\\n\".format(classify.accuracy(classifier, train_data)))\n",
    "print(\"Testing accuracy is:{}\\n\".format(classify.accuracy(classifier, test_data)))\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom testing\n",
    "\n",
    "For ease of use we wrap our classification algorithm into a function, then we perform tests on various emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(tweet):\n",
    "    '''\n",
    "    Wrapper function for the pre-processing and classification steps previously performed.\n",
    "    \n",
    "    @arg tweet: String representing a tweet\n",
    "    @return String representing a polarity. (Positive or Negative)\n",
    "    '''\n",
    "    tokens = tweet_tokenizer.tokenize(tweet)\n",
    "    tokens = [\n",
    "        lemmatizer.lemmatize(word, _tag2type(tag)).lower()\n",
    "        for word, tag in pos_tag(tokens)\n",
    "        if not _is_noise(word)\n",
    "    ]\n",
    "    \n",
    "    return tokens, classifier.classify(dict([token, True] for token in tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Denoised tokens: ['thanks', 'pie', 'really', 'appreciate', ':)', '#yummy', '#pie_day']\n",
      "Polarity: Positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "positive_tweet = \"@bakery_brothers Thanks for the Pie! Really appreciate it :) #yummy #pie_day\"\n",
    "tokens, polarity = classify(positive_tweet)\n",
    "\n",
    "print(\"Denoised tokens: {}\\nPolarity: {}\\n\".format(tokens, polarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Denoised tokens: ['really', 'sad', 'lose', 'qualification', 'final', '#no_luck']\n",
      "Polarity: Negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "negative_tweet = \"@raptors really sad that you lost the qualifications to the final. #no_luck\"\n",
    "tokens, polarity = classify(negative_tweet)\n",
    "\n",
    "print(\"Denoised tokens: {}\\nPolarity: {}\\n\".format(tokens, polarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Denoised tokens: ['thank', 'much', 'close', 'half', 'road', 'city', 'middle', 'day', '#traffic']\n",
      "Polarity: Positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sarcasme_tweet = \"@police thank you so much for closing half the roads to the city in the middle of the day! #traffic\"\n",
    "tokens, polarity = classify(sarcasme_tweet)\n",
    "\n",
    "print(\"Denoised tokens: {}\\nPolarity: {}\\n\".format(tokens, polarity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "The model is not able to recognize sarcasme for lack of data in the training set.\n",
    "\n",
    "Training a more complex model that would recognize more evolved emotions requires a training set that contains all of those emotions, and eventually a classification algorithm that can cope with this complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Save the Model into Binary File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./model.pickle', 'wb') as f:\n",
    "    pickle.dump(classifier, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
